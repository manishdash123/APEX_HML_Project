{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xml.etree.ElementTree as ET\n",
    "import xml.dom.minidom\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Generating xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1 Preprocess CSV file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessed CSV file created: preprocessed_output.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def preprocess_csv(input_file, output_file):\n",
    "    \"\"\"\n",
    "    Processes the CSV file by mapping each of the timestep values to unique integers.\n",
    "\n",
    "    Parameters:\n",
    "    input_file (str): The path to the input CSV file. //Might update with systemn\n",
    "    output_file (str): The path to the output preprocessed CSV file. //Chage this to store the csv somewhere else\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    timestep_mapping = {} # Dictionary to map each timestep to a unique integer\n",
    "    current_timestep = 0 \n",
    "    \n",
    "    with open(input_file, 'r') as file:\n",
    "        csv_reader = csv.reader(file) # Read the CSV file\n",
    "        rows = list(csv_reader) # Store the rows of the CSV in a list\n",
    "    \n",
    "    with open(output_file, 'w', newline='') as file:\n",
    "        csv_writer = csv.writer(file) # Write to the output CSV file\n",
    "\n",
    "        #Iterate through each row in the CSV file\n",
    "        for row in rows:\n",
    "            timestep = float(row[0]) # Extract the timestep value from the row\n",
    "            \n",
    "            if timestep not in timestep_mapping: # If the timestep is not already mapped\n",
    "                timestep_mapping[timestep] = current_timestep # Map the timestep to the current current_timestep equal \n",
    "                current_timestep += 1  # Increment the current_timestep\n",
    "            \n",
    "            row[0] = str(timestep_mapping[timestep]) # Update the timestep value in the row\n",
    "            csv_writer.writerow(row) \n",
    "\n",
    "# input and output CSV file paths\n",
    "input_file = 'build/bin/output.csv'\n",
    "output_file = 'preprocessed_output.csv'\n",
    "\n",
    "# Preprocess the CSV file\n",
    "preprocess_csv(input_file, output_file)\n",
    "print(f\"Preprocessed CSV file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2 Create Graphs list from Preprocessed CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the preprocessed CSV file\n",
    "df = pd.read_csv('preprocessed_output.csv', header=None) #Header row doesnt exists\n",
    "df.columns = ['Timestep', 'chunkID', 'source', 'destination']\n",
    "\n",
    "# print(df.head) #Print the first 5 rows of the dataframe. Use for debugging\n",
    "\n",
    "# Assuming height and width of the mesh are known\n",
    "height = 3  # Adjust as necessary based on mesh structure\n",
    "width = 3   # Adjust as necessary based on mesh structure\n",
    "\n",
    "def add_all_nodes(G, height, width):\n",
    "    \"\"\"\n",
    "    Adds all nodes to the graph G.\n",
    "\n",
    "    Parameters:\n",
    "    - G (networkx.Graph): The graph to which the nodes will be added.\n",
    "    - height (int): The height of the grid.\n",
    "    - width (int): The width of the grid.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    for i in range(height * width):  # Assuming node identifiers are from 0 to height*width-1\n",
    "        G.add_node(i) #in all the mesh, add nodes\n",
    "\n",
    "# Get unique timesteps\n",
    "timesteps = df['Timestep'].unique()\n",
    "\n",
    "graphs = []\n",
    "\n",
    "for timestep in timesteps:\n",
    "    timestep_data = df[df['Timestep'] == timestep] # slice the dataframes into t = 0, 1...N\n",
    "    print(timestep_data.head)\n",
    "    \n",
    "    G = nx.DiGraph() #Create a directed graph\n",
    "    add_all_nodes(G, height, width)  # Add nodes for this timestep\n",
    "    \n",
    "    # Directly use integer identifiers from the CSV\n",
    "    for _, row in timestep_data.iterrows(): #Iterate through the rows of the dataframe\n",
    "        source = row['source'] #Extract the source node\n",
    "        print('src = ',source) \n",
    "        destination = row['destination'] #Extract the destination node\n",
    "        print('dst = ', destination)\n",
    "        chunkID = row['chunkID']    #Extract the chunkID\n",
    "        print('chunk = ', chunkID)\n",
    "        G.add_edge(source, destination, chunkID=chunkID) #Add these nodes as the edge to the graph\n",
    "    \n",
    "    graphs.append(G) #Append this graph to the list of graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Step 1.3 Create data structure node_traffic from graphs list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node 0 Thread Blocks:\n",
      "Edge (0, 1) with Thread ID 0:\n",
      "{'timestep': 0, 'chunkID': 0, 'type': 's', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 1, 'chunkID': 3, 'type': 's', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 2, 'chunkID': 6, 'type': 's', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 3, 'chunkID': -1, 'type': '-1', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 4, 'chunkID': -1, 'type': '-1', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "Edge (0, 3) with Thread ID 1:\n",
      "{'timestep': 0, 'chunkID': 0, 'type': 's', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 1, 'chunkID': 1, 'type': 's', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 2, 'chunkID': -1, 'type': '-1', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 3, 'chunkID': 2, 'type': 's', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 4, 'chunkID': -1, 'type': '-1', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "Edge (1, 0) with Thread ID 2:\n",
      "{'timestep': 0, 'chunkID': 1, 'type': 'r', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 1, 'chunkID': 4, 'type': 'r', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 2, 'chunkID': 2, 'type': 'r', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 3, 'chunkID': 5, 'type': 'r', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 4, 'chunkID': -1, 'type': '-1', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "Edge (3, 0) with Thread ID 3:\n",
      "{'timestep': 0, 'chunkID': 3, 'type': 'r', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 1, 'chunkID': 6, 'type': 'r', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 2, 'chunkID': 7, 'type': 'r', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 3, 'chunkID': 8, 'type': 'r', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n",
      "{'timestep': 4, 'chunkID': -1, 'type': '-1', 'deps': -1, 'depsid': -1, 'hasdeps': 0}\n"
     ]
    }
   ],
   "source": [
    "# Normally you would load data from a file like so:\n",
    "# df = pd.read_csv(\"path_to_your_file.csv\")\n",
    "# Load the preprocessed CSV file\n",
    "df = pd.read_csv('preprocessed_output.csv', header=None) #Header row doesnt exists\n",
    "df.columns = ['Timestep', 'chunkID', 'source', 'destination']\n",
    "\n",
    "# Create graphs per timestep\n",
    "graphs = {t: nx.DiGraph() for t in df['Timestep'].unique()}\n",
    "\n",
    "# Populate each graph\n",
    "for _, row in df.iterrows():\n",
    "    graphs[row['Timestep']].add_edge(row['source'], row['destination'], chunkID=row['chunkID'])\n",
    "\n",
    "# Initialize node_traffic from all possible edges in the first graph\n",
    "node_traffic = {}\n",
    "\n",
    "first_graph = graphs[0]  # Assuming the first timestep graph is fully representative\n",
    "for node in first_graph.nodes():\n",
    "    \n",
    "    node_traffic[node] = {}\n",
    "    edge_counter = 0  # Reset counter for each node\n",
    "\n",
    "    # Add thread blocks for all outgoing edges\n",
    "    for target in first_graph.successors(node):\n",
    "        node_traffic[node][(node, target)] = {\n",
    "            'threadID': edge_counter,\n",
    "            'records': [{'timestep': t, 'chunkID': -1, 'type': '-1', 'deps': -1, 'depsid': -1, 'hasdeps': 0} for t in graphs.keys()]\n",
    "        }\n",
    "        edge_counter += 1\n",
    "\n",
    "    # Add thread blocks for all incoming edges if not already added\n",
    "    for source in first_graph.predecessors(node):\n",
    "        if (source, node) not in node_traffic[node]:\n",
    "            node_traffic[node][(source, node)] = {\n",
    "                'threadID': edge_counter,\n",
    "                'records': [{'timestep': t, 'chunkID': -1, 'type': '-1', 'deps': -1, 'depsid': -1, 'hasdeps': 0} for t in graphs.keys()]\n",
    "            }\n",
    "            edge_counter += 1\n",
    "\n",
    "# Populate the node_traffic with actual data from all timestep graphs\n",
    "for t_index, graph in graphs.items():\n",
    "    for source, destination, data in graph.edges(data=True):\n",
    "        node_traffic[source][(source, destination)]['records'][t_index]['chunkID'] = data['chunkID']\n",
    "        node_traffic[source][(source, destination)]['records'][t_index]['type'] = 's'\n",
    "        node_traffic[destination][(source, destination)]['records'][t_index]['chunkID'] = data['chunkID']\n",
    "        node_traffic[destination][(source, destination)]['records'][t_index]['type'] = 'r'\n",
    "\n",
    "# Debug: Print out node_traffic for node 0 to see thread blocks\n",
    "print(\"Node 0 Thread Blocks:\")\n",
    "for edge_key, tb_info in node_traffic[0].items():\n",
    "    print(f\"Edge {edge_key} with Thread ID {tb_info['threadID']}:\")\n",
    "    for record in tb_info['records']:\n",
    "        print(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_graph = graphs[0]  # Assuming the first timestep graph is fully representative\n",
    "for node in first_graph.nodes():\n",
    "    print(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(node_traffic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_node_traffic(data, indent=0):\n",
    "    for node, edges in data.items():\n",
    "        print('    ' * indent + str(node) + ':')\n",
    "        if isinstance(edges, dict):\n",
    "            pretty_print_node_traffic(edges, indent + 1)\n",
    "        else:\n",
    "            print('    ' * (indent + 1) + str(edges))\n",
    "\n",
    "pretty_print_node_traffic(node_traffic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.4 Generate XML from data structure node_traffic & dump in XML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunks/NPU: 1\n",
      "Number of chunks = 9\n"
     ]
    }
   ],
   "source": [
    "def parse_output_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        # Using regular expressions to find the desired pattern\n",
    "        pattern = r'\\[All-Reduce Information\\]\\s*#Chunks/NPU:\\s*(\\d+)'\n",
    "        num_chunks = r'\\[2D Mesh Information\\]\\s*#NPUs:\\s*(\\d+)'\n",
    "        match_chunks_per_npu, match_num_chunks = re.search(pattern, content), re.search(num_chunks, content)\n",
    "        if match_chunks_per_npu and match_num_chunks:\n",
    "            # Extracting the value of #Chunks/NPU\n",
    "            chunks_per_npu, chunks_num = int(match_chunks_per_npu.group(1)), int(match_num_chunks.group(1))\n",
    "            return chunks_per_npu, chunks_num\n",
    "        else:\n",
    "            # If the pattern is not found\n",
    "            return None, None\n",
    "\n",
    "# Example usage:\n",
    "file_path = \"./TACOS_output.txt\" # path to the file (from ./run.sh)\n",
    "chunks_per_npu, num_chunks = parse_output_file(file_path)\n",
    "if chunks_per_npu is not None:\n",
    "    print(\"Chunks/NPU:\", chunks_per_npu)\n",
    "    print(f\"Number of chunks = {num_chunks}\")\n",
    "\n",
    "# Function to generate and save XML in a pretty format\n",
    "def generate_and_save_xml_pretty(node_traffic, filename, chunks_per_npu, num_chunks):\n",
    "\n",
    "    root = ET.Element(\"network\")\n",
    "\n",
    "    # Create <gpu> tags for each node\n",
    "    for node, edges in sorted(node_traffic.items()):\n",
    "        gpu_elem = ET.SubElement(root, \"gpu\", id=str(node), i_chunks=\"0\", o_chunks=str(num_chunks), s_chunks=\"0\")\n",
    "\n",
    "        # Create <tb> tags for each connected edge\n",
    "        for (source, destination), edge_info in edges.items():\n",
    "            tb_elem = ET.SubElement(gpu_elem, \"tb\", id=str(edge_info['threadID']),\n",
    "                                    send=str(destination if edge_info['records'][0]['type'] == 's' else '-1'),\n",
    "                                    recv=str(source if edge_info['records'][0]['type'] == 'r' else '-1'),\n",
    "                                    chan =\"0\")\n",
    "            \n",
    "            # create <step> tags for eeach time step\n",
    "            for record in edge_info['records']:\n",
    "                ET.SubElement(tb_elem, \"step\", s=str(record['timestep']), type=record['type'], srcbuf=\"o\", srcoff=str(record['chunkID']), dstbuf=\"o\", dstoff=str(record['chunkID']), cnt= str(chunks_per_npu), deps=str(record['deps']), depsid=str(record['depsid']), hasdeps=str(record['hasdeps']))\n",
    "\n",
    "    # Convert to string using ElementTree and parse with minidom for pretty printing\n",
    "    rough_string = ET.tostring(root, 'utf-8')\n",
    "    reparsed = xml.dom.minidom.parseString(rough_string)\n",
    "    pretty_xml_as_string = reparsed.toprettyxml(indent=\"  \")\n",
    "\n",
    "    # Write to file\n",
    "    with open(filename, 'w') as file:\n",
    "        file.write(pretty_xml_as_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML file saved as network_flow_pretty.xml\n"
     ]
    }
   ],
   "source": [
    "# Usage\n",
    "filename = 'network_flow_pretty.xml'\n",
    "generate_and_save_xml_pretty(node_traffic, filename, chunks_per_npu, num_chunks)\n",
    "print(f\"XML file saved as {filename}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
